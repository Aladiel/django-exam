services:
  web:
    build: .
    container_name: make-your-cocktail
    command: >
      sh -c "python manage.py migrate &&
             python manage.py collectstatic --noinput &&
             gunicorn Make_your_cocktAIl.wsgi:application --bind 0.0.0.0:8000 --workers 2 --threads 4 --timeout 120 --graceful-timeout 30"
    ports:
      - "8000:8000"
    env_file:
      - .env.demo
    environment:    # prod ou dev
      - MODE=dev
      - OLLAMA_HOST=http://ollama:11434
    volumes:
      - .:/app
      - static_volume:/app/static
      - media_volume:/app/media/cocktails
    depends_on:
      - ollama
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    entrypoint: >
      sh -c "
      ollama serve &
      sleep 5 &&
      ollama pull llama3:8b &&
      wait
      "
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    restart: unless-stopped

volumes:
  static_volume:
  media_volume:
  ollama_models: